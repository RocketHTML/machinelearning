Intro to Machine Learning: https://www.youtube.com/watch?v=h0e2HAPTGF4

<colored clustering excerpt>
"The image above represents the clusters found by a clustering algorithm. The arms of each color equal the distance from the center to each point in the cluster. (Courtesy of Ana Bell.)"

Linear Regression:
"Let's say I have some experimental data. Regression gives us a way to deduce a model to fit that data. In some cases it's easy, for example when we know it's going to be a linear model we can find the best line to fit that data, and in other cases we can actually use validation to help us explore and find the best model that would fit it: whether it's a linear, a quadratic, a cubic, or some higher order thing. You'll see that understanding regression provides a nice segway into understanding machine learning."
<show a linear and quadratic data relationship>

Related topics
"Machine learning is a huge topic within itself, not including the various subjects in which learning plays a central part: natural language processing, computational biology, computer vision, robotics, all rely today heavily on machine learning and you'll see it in those subjects."

Outline
"So we're not going to compress several sbujects into one article, but instead we're going to start by talking about the basic concepts of machine learning: the idea of having examples, and how do you talk about features representing those examples. How do you measure distances between them, and use the notion of distance as a way to group similar things together, as a way of doing machine learning. And we're gonna look, as a consequence, of two different standard ways of doing learning: Classification methods and Clustering methods. Classification works well when I have what we could call labeled data. I know labels on my examples, and I'm going to use that to try to defind classes that I can learn. Clustering working well when I don't have labeled data. Again we're not going to go into the current really sophisticated machine learning methods, like convolutional learning nets or deep learning, things you read about in the news. But you're going to get a sense of what's behind those by looking at what we do when we talk about learning algorithms."

<picture of various public uses of machine learning> 5:20

Basics - What is machine learning?
"You could argue that almost every computer program learns something. But the level of learning really varies alot. So for example, imagine a function that computes square roots. You could argue, you'd have to stretch it, but you could argue that the function learns something about computing square roots. In fact you could generalize it to compute roots of any order power. But it really isn't learning, I had to program it. <pic of input and square root output> 

But now think about linear regression. You start out with a set of data points, mass displacement data points, and then the computer essentially fits a curve to that data point, and it was in some sense learning a model for that data that it could then use to predict data in other situations. And that's getting closer to what we would like when we think about a machine learning algorithm; we would like to have a program that can learn from experience, that it can then use to deduce new facts."

<quote>Early defition of machine learning: "Field of study that gives computers the ability to learn without being explicitly programmed." Arthur Samuel [1959] </quote>

Many people would argue he wrote the first such program. It learned from experience. In his case it played checkers, which kind of shows you how the field has progressed; we started with checkers, we got to chess, we now do Go. But it played checkers, it beat national level players, most importantly it learned to improve its methods by watching how it did in games and then inferring something to change what it thought about as it did that. (Samuel did a bunch of other things. He invented something known as alpha-beta pruning, which is a really useful method of doing search.)

So the idea is how can we have the computer learn without being explicitly programmed. And one way to think about this is to think about the difference between how we would normally program and what we would like from a machine learning algorithm. 

Normal Programming:
I know you're not convinced there's such a thing as normal programming, but if you think of traditional programming, what's the process? I write a program, that I input to the computer, so that it can then take data and produce some appropriate data. The square root finder really sits there, right? I wrote code for using a method to find a square root, and that gave me the process of: given any number, I'll give you the square root.
<data output pics> 8:30

But in a machine learning approach, the idea is that I'm going to give the computer output, I'm going to give it examples of what I want the program to do: labels on data, characterizations on different classes of things. And what I want the computer to do, is given that characterization of output and data, I want that machine learning algorithm to actually produce a program for me. A program that I can then use to infer new information about things. And that creates, if you like, a really nice loop, where I can have the machine learning algorithm learn the program which I can then use to solve some other problem. 
<swerve data output pic> 9:30

And that would be really great if we could do it. And that curve fitting algorithm is a simple version of that; it learns a model for the data, which I can then use to label any other instances of the data, or predict what I would see in terms of spring displacement as I change the masses. So that's the kind of idea we're going to explore.

How do you learn?
So if we want to learn things then we could also ask: How do you learn? And how should a computer learn? For you as a human, there are a couple of possibilities. The first is the boring one, the old style way, memorize facts. Memorize as many facts as you can and then hope that on the exam we ask you instances of those facts, as opposed to some other facts that you haven't memorized. This is an example of Declaritive Knowledge, memorize as much as you can, have wikipedia in your backpocket. Better way to learn is to be able to infer, to be able to deduce new information from old. And if you think about this, this gets closer to Imparitive Knowledge, ways to deduce new things. 

In the memorization case we built that in, with the square root algorithm. But what we'd like to have in a learning algorithm, is to have it infer things based on what it's seen so far. We're interested in extending our capabilities to write programs that can infer useful information from implicit patterns in the data, so not something explicitly built-in like a comparison of weights and displacements, but actual implicit patterns in the data, and have the algorithm figure out what those patterns are, and use those to generate a program that you can use to infer new data about objects, spring displacements, or whatever it is you're trying to do. 

Basic Paradigm
So the idea then, the basic paradigm, is that we're going to give the system some training data, some observations like spring displacements. We're going to then try to figure out how to write code, how to write a system that will infer something about the process that generated the data. Then from that, we want to be able to use that to make predictions about things we haven't seen before. So to drive home this point, if you think about it, the regression examples fit that model. Given a set of data that we already know has a linear relationship, for different x's how far does the y move? We then inferred something about the underlying process, we inferred the linear equation, the constant associated with it. Based on that, I got a piece of code that I can use to predict new displacements. So its got all of the paradigm elements: training data, inference engine, and then the ability to use that to make new predictions.

But that's a very simple kind of learning setting. The more common one is: say I give you a set of examples, and those examples have some data associated with them, some features. And the examples have some labels, so for each example I might say 'this is a particular kind of thing, this other one is another kind of thing', and what I want to do is figure out how to do inference on labels of things. So it's not just what's the displacement of the masses, each point in the mass also has a label or a catagory it belongs to. 

13:15
Let's go through an example with some football players. 



Classification methods
- k nearest neighbor

Clustering methods
- k-means